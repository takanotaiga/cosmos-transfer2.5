# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import sys
from dataclasses import dataclass
from pathlib import Path

import yaml

# Add the parent directory of 'vqa' to Python path to allow absolute imports to work when running from any location
# This allows running the script as:
#   - python packages/cosmos-oss/vqa/vqa_evaluator.py (from repo root)
#   - python vqa/vqa_evaluator.py (from ci_cd directory)
#   - python vqa_evaluator.py (from vqa directory)
vqa_dir = Path(__file__).parent.resolve()  # .../packages/cosmos-oss/vqa
parent_dir = vqa_dir.parent.resolve()  # .../packages/cosmos-oss (or wherever vqa's parent is)

print(f"Adding {parent_dir} to Python path")
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

from vqa.cosmos_reason_inference import CosmosReasonModel  # noqa: E402


@dataclass
class VQACheck:
    """
    Represents a single VQA check with question, expected answer, and validation keywords.

    Attributes:
        question: The question to ask about the video
        answer: Expected answer (for reference/documentation)
        contains: List of keywords that the actual answer should contain for validation
    """

    question: str
    answer: str
    contains: list[str]


def parse_vqa_checks_from_yaml(test_config_path: str | Path) -> list[VQACheck]:
    """
    Parse VQA checks from a YAML file.

    Args:
        test_config_path: Path to the YAML file containing VQA checks

    Returns:
        List of VQACheck objects

    Expected YAML format:
        vqa_checks:
          - question: What type of environment is the video set in?
            answer: A modern, well-lit office
            contains:
              - "modern"
              - "office"
              - "well-lit"
          - question: What is the main focus of the video?
            answer: A robotic interaction at a counter
            contains:
              - "robotic"
              - "interaction"
    """
    test_config_path = Path(test_config_path)

    if not test_config_path.exists():
        raise FileNotFoundError(f"Test config file not found: {test_config_path}")

    with test_config_path.open("r") as f:
        data = yaml.safe_load(f)

    if not data or "vqa_checks" not in data:
        raise ValueError("YAML file must contain 'vqa_checks' key")

    vqa_checks = []
    for check_data in data["vqa_checks"]:
        if "question" not in check_data:
            raise ValueError("Each VQA check must have a 'question' field")

        vqa_check = VQACheck(
            question=check_data["question"],
            answer=check_data.get("answer", ""),
            contains=check_data.get("contains", []),
        )
        vqa_checks.append(vqa_check)

    return vqa_checks


@dataclass
class VQAResult:
    """
    Represents the result of a VQA check.

    Attributes:
        check: The original VQACheck object
        actual_answer: The actual answer generated by the model
        validation_passed: Whether the answer contains all required keywords
        missing_keywords: List of keywords that were not found in the answer
    """

    check: VQACheck
    actual_answer: str
    validation_passed: bool
    missing_keywords: list[str]


def validate_answer(actual_answer: str, expected_keywords: list[str]) -> tuple[bool, list[str]]:
    """
    Validate if the actual answer contains at least one of the expected keywords.

    Args:
        actual_answer: The answer generated by the model
        expected_keywords: List of keywords to check for (case-insensitive)

    Returns:
        Tuple of (validation_passed, missing_keywords)
        - validation_passed: True if at least ONE keyword is found
        - missing_keywords: List of keywords that were not found
    """
    if not expected_keywords:
        # If no keywords specified, validation passes
        return True, []

    actual_answer_lower = actual_answer.lower()
    missing_keywords = []
    found_keywords = []

    for keyword in expected_keywords:
        if keyword.lower() in actual_answer_lower:
            found_keywords.append(keyword)
        else:
            missing_keywords.append(keyword)

    # Pass validation if at least one keyword is found
    validation_passed = len(found_keywords) > 0
    return validation_passed, missing_keywords


def run_vqa_batch(
    video_path: str | Path,
    test_config_path: str | Path,
    model_name: str | None = None,
    revision: str | None = None,
    validate: bool = True,
    verbose: bool = False,
) -> list[VQAResult]:
    """
    Run VQA inference on a video with multiple questions from a YAML file.

    Args:
        video_path: Path to the video file
        test_config_path: Path to the YAML file containing VQA checks
        model_name: HuggingFace model identifier (default: None, uses CosmosReasonModel.MODEL_NAME)
        revision: Model revision (branch name, tag name, or commit id)
        validate: Whether to validate answers against expected keywords (default: True)
        verbose: Whether to print detailed output during inference (default: False)

    Returns:
        List of VQAResult objects containing questions, answers, and validation results
    """
    # Parse VQA checks from YAML
    print(f"Loading VQA checks from: {test_config_path}")
    vqa_checks = parse_vqa_checks_from_yaml(test_config_path)
    print(f"Found {len(vqa_checks)} VQA checks")

    # Initialize model
    print("\nInitializing Cosmos Reason model...")
    model = CosmosReasonModel(model_name=model_name, revision=revision)
    print(f"Model loaded: {model.model_name}")

    # Run inference for each VQA check
    results: list[VQAResult] = []
    print(f"\nProcessing video: {video_path}")
    print("=" * 80)

    for idx, vqa_check in enumerate(vqa_checks, 1):
        print(f"\n[Question {idx}/{len(vqa_checks)}]")
        print(f"Q: {vqa_check.question}")
        print(f"Expected: {vqa_check.answer}")

        try:
            actual_answer = model.infer(
                video_path=str(video_path),
                question=vqa_check.question,
                verbose=verbose,
            )
            print(f"Actual: {actual_answer}")

            # Validate answer if requested
            if validate and vqa_check.contains:
                validation_passed, missing_keywords = validate_answer(actual_answer, vqa_check.contains)
                print(f"Validation: {'✓ PASSED' if validation_passed else '✗ FAILED'}")
                if missing_keywords:
                    print(f"Missing keywords: {', '.join(missing_keywords)}")
            else:
                validation_passed = True
                missing_keywords = []

            result = VQAResult(
                check=vqa_check,
                actual_answer=actual_answer,
                validation_passed=validation_passed,
                missing_keywords=missing_keywords,
            )
            results.append(result)

        except Exception as e:  # noqa: BLE001 - Catch all errors to continue processing remaining checks
            error_msg = f"Error: {e!s}"
            print(f"Actual: {error_msg}")
            print("Validation: ✗ ERROR")

            result = VQAResult(
                check=vqa_check,
                actual_answer=error_msg,
                validation_passed=False,
                missing_keywords=vqa_check.contains,
            )
            results.append(result)

    # Print summary
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    print(f"Total checks: {len(results)}")

    if validate:
        passed = sum(1 for r in results if r.validation_passed)
        failed = len(results) - passed
        print(f"Passed: {passed}")
        print(f"Failed: {failed}")
        print(f"Success rate: {passed / len(results) * 100:.1f}%")

    return results


def evaluate_success_rate(output_path: str | Path, threshold: float = 80.0) -> bool:
    """
    Evaluate the success rate from a VQA results JSON file.

    Args:
        output_path: Path to the JSON output file
        threshold: Success threshold percentage (default: 80.0)

    Returns:
        True if success rate >= threshold, False otherwise

    Raises:
        FileNotFoundError: If the output file doesn't exist
        ValueError: If the JSON file is invalid or missing required fields
    """
    import json

    output_path = Path(output_path)

    if not output_path.exists():
        raise FileNotFoundError(f"Output file not found: {output_path}")

    with output_path.open("r") as f:
        data = json.load(f)

    # Validate required fields
    if "total_checks" not in data or "passed" not in data:
        raise ValueError("JSON file must contain 'total_checks' and 'passed' fields")

    total_checks = data["total_checks"]
    passed = data["passed"]

    if total_checks == 0:
        raise ValueError("Cannot evaluate success rate: total_checks is 0")

    # Calculate success rate
    success_rate = (passed / total_checks) * 100.0

    # Determine if it meets threshold
    is_success = success_rate >= threshold

    # Print results
    print("\n" + "=" * 80)
    print("VQA SUCCESS RATE EVALUATION")
    print("=" * 80)
    print(f"Total Checks: {total_checks}")
    print(f"Passed: {passed}")
    print(f"Failed: {data.get('failed', total_checks - passed)}")
    print(f"Success Rate: {success_rate:.2f}%")
    print(f"Threshold: {threshold:.2f}%")
    print("-" * 80)
    if is_success:
        print(f"✓ SUCCESS: Success rate ({success_rate:.2f}%) meets threshold ({threshold:.2f}%)")
    else:
        print(f"✗ FAILURE: Success rate ({success_rate:.2f}%) below threshold ({threshold:.2f}%)")
    print("=" * 80)

    return is_success


def main() -> None:
    """Main entry point for CLI usage."""
    parser = argparse.ArgumentParser(description="Run batch VQA inference on a video with questions from a YAML file")
    parser.add_argument(
        "--video_path",
        type=str,
        required=True,
        help="Path to the input video file",
    )
    parser.add_argument(
        "--test_config_path",
        type=str,
        required=True,
        help="Path to the YAML test configuration file containing VQA checks",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default=None,
        help=f"HuggingFace model identifier (default: {CosmosReasonModel.MODEL_NAME})",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        help="Model revision (branch name, tag name, or commit id)",
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        default=True,
        help="Validate answers against expected keywords (default: True)",
    )
    parser.add_argument(
        "--no-validate",
        action="store_false",
        dest="validate",
        help="Disable answer validation",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Verbose output during inference",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="outputs/vqa_results.json",
        help="Optional path to save results as JSON file",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=80.0,
        help="Success rate threshold percentage (default: 80.0)",
    )
    parser.add_argument(
        "--evaluate-only",
        type=str,
        default=None,
        metavar="JSON_FILE",
        help="Evaluate success rate from existing JSON file without running inference",
    )

    args = parser.parse_args()

    # Evaluate-only mode
    if args.evaluate_only:
        is_success = evaluate_success_rate(args.evaluate_only, threshold=args.threshold)
        exit(0 if is_success else 1)

    # Run batch VQA
    results = run_vqa_batch(
        video_path=args.video_path,
        test_config_path=args.test_config_path,
        model_name=args.model_name,
        revision=args.revision,
        validate=args.validate,
        verbose=args.verbose,
    )

    # Optionally save results to file
    if args.output:
        import json

        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Convert results to serializable format
        results_dict = []
        for result in results:
            results_dict.append(
                {
                    "question": result.check.question,
                    "expected_answer": result.check.answer,
                    "actual_answer": result.actual_answer,
                    "expected_keywords": result.check.contains,
                    "validation_passed": result.validation_passed,
                    "missing_keywords": result.missing_keywords,
                }
            )

        output_data = {
            "video_path": str(args.video_path),
            "test_config_path": str(args.test_config_path),
            "total_checks": len(results),
            "passed": sum(1 for r in results if r.validation_passed),
            "failed": sum(1 for r in results if not r.validation_passed),
            "results": results_dict,
        }

        with output_path.open("w") as f:
            json.dump(output_data, f, indent=2)

        print(f"\nResults saved to: {args.output}")

        # Evaluate success rate
        is_success = evaluate_success_rate(args.output, threshold=args.threshold)
        exit(0 if is_success else 1)
    else:
        # Exit with appropriate code based on validation results
        if args.validate:
            passed = sum(1 for r in results if r.validation_passed)
            success_rate = (passed / len(results)) * 100.0
            is_success = success_rate >= args.threshold
            exit(0 if is_success else 1)


if __name__ == "__main__":
    main()
